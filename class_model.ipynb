{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ff5d7f",
   "metadata": {},
   "source": [
    "<div class='alert alert-success'>\n",
    "\n",
    "**JAX IMPLEMENTATION FOR CLASSIFICATION**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f221f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa029a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "def load_data(test_test=0.3, random_state=42):\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "    \n",
    "    #test-train split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_test, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    #Standardizing\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3c9db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "class MLP(nn.Module):\n",
    "    #Number of hidden units\n",
    "    features: int\n",
    "    \n",
    "    @nn.compact #Flax magic for concise layer definitions. \n",
    "    #Instead explicitly defining submodules in a separate method, \n",
    "    #this allows writing the network's logic in a single forward pass.\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.features)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(3)(x) #For the 3 classes\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c271539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def cross_entropy_loss(logits, labels):\n",
    "    #Converts labels into one-hot vectors \n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=3)\n",
    "    #Compute and return the softmax cross-entropy\n",
    "    return optax.softmax_cross_entropy(logits, one_hot_labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba763c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, lr=0.01):\n",
    "    #Model initialization \n",
    "    model = MLP(features=10) #i.e. 10 hidden states\n",
    "    #Input shape is (batch_size, 4 features)\n",
    "    init_rng, _ = jax.random.split(rng)\n",
    "    params = model.init(init_rng, jnp.ones([1, 4]))['params']\n",
    "    \n",
    "    #Optimizer\n",
    "    tx = optax.adam(lr)\n",
    "    \n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply, #Function to apply the model\n",
    "        params=params, #Initial parameters\n",
    "        tx=tx #Optimizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfcf8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "@jax.jit #compile to XLA for faster execution\n",
    "#This means that just-in-time compilation speeds up the execution by\n",
    "#compiling function to XLA(Accelerated Linear Algebra)\n",
    "#Do to optimize performance, reduce overhead, and cache results\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch['features'])\n",
    "        loss = cross_entropy_loss(logits, batch['labels'])\n",
    "        return loss, logits #Returning the loss for gradient computations\n",
    "    \n",
    "    #Gradient computations and parameter updates\n",
    "    #Compute loss + gradients\n",
    "    #value_and_grad is JAX's auto-diff to compute gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True) \n",
    "    (loss, logits), grads =  grad_fn(state.params)\n",
    "    \n",
    "    #update parameters using the gradients\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "def train_epoch(state, train_data, batch_size=32):\n",
    "    #Convert to JAX arrays\n",
    "    features = jnp.array(train_data[0])\n",
    "    labels = jnp.array(train_data[1])\n",
    "    \n",
    "    assert labels.ndim==1, \"Labels must be 1D\"\n",
    "    \n",
    "    #Batches creation\n",
    "    #Shuffling for randomness i.e., jax.random.permutation\n",
    "    dataset_size = features.shape[0]\n",
    "    indices = jax.random.permutation(jax.random.PRNGKey(0), dataset_size)\n",
    "    \n",
    "    #Processing data in batches\n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch = {\n",
    "            'features': features[indices[start_idx:end_idx]],\n",
    "            'labels': labels[indices[start_idx:end_idx]]\n",
    "        }\n",
    "        #Sanity checks\n",
    "        print(f\"Batch features shape: {batch['features'].shape}\")\n",
    "        print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
    "        state, loss = train_step(state, batch)\n",
    "        \n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebf30441",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "jit() missing 1 required positional argument: 'fun'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Evaluation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit(static_argnames\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapply_fn\u001b[39m\u001b[38;5;124m'\u001b[39m,))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(params, apply_fn, features, labels):\n\u001b[0;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m apply_fn({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params}, features) \u001b[38;5;66;03m#The forward pass\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Class with highes score\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: jit() missing 1 required positional argument: 'fun'"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "@jax.jit(static_argnames=('apply_fn',))\n",
    "def compute_accuracy(params, apply_fn, features, labels):\n",
    "    logits = apply_fn({'params': params}, features) #The forward pass\n",
    "    predicted = jnp.argmax(logits, axis=1) #Class with highes score\n",
    "    return jnp.mean(predicted == labels) #Accuracy\n",
    "\n",
    "\n",
    "#jnp.argmax convert logits to class predictions\n",
    "#jnp.mean calculate the fraction of correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c27fa95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (105, 4)\n",
      "y_train shape: (105,)\n",
      "X_test shape: (45, 4)\n",
      "y_test shape: (45,)\n",
      "X_train shape: (105, 4)\n",
      "X_test shape: (45, 4)\n",
      "Batch features shape: (32, 4)\n",
      "Batch labels shape: (32,)\n",
      "Batch features shape: (32, 4)\n",
      "Batch labels shape: (32,)\n",
      "Batch features shape: (32, 4)\n",
      "Batch labels shape: (32,)\n",
      "Batch features shape: (9, 4)\n",
      "Batch labels shape: (9,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function compute_accuracy at 0x00000205C7FCA5C0> as an abstract array. The problematic value is of type <class 'method'> and was passed to the function at path apply_fn.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#Print metrics for every 10 epochs\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 22\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m compute_accuracy(\n\u001b[0;32m     23\u001b[0m         state\u001b[38;5;241m.\u001b[39mparams, state\u001b[38;5;241m.\u001b[39mapply_fn, X_train, y_train\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m compute_accuracy(\n\u001b[0;32m     26\u001b[0m         state\u001b[38;5;241m.\u001b[39mparams, state\u001b[38;5;241m.\u001b[39mapply_fn, X_test, y_test\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rajan\\anaconda3\\Lib\\site-packages\\jax\\_src\\pjit.py:739\u001b[0m, in \u001b[0;36m_infer_input_type\u001b[1;34m(fun, dbg, explicit_args)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    738\u001b[0m   arg_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg\u001b[38;5;241m.\u001b[39marg_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 739\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    741\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and was passed to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument, and this argument was not marked as static using the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    746\u001b[0m   ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmutable_array_checks\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    748\u001b[0m   _check_no_aliased_ref_args(dbg, avals, explicit_args)\n",
      "\u001b[1;31mTypeError\u001b[0m: Error interpreting argument to <function compute_accuracy at 0x00000205C7FCA5C0> as an abstract array. The problematic value is of type <class 'method'> and was passed to the function at path apply_fn.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "#Main\n",
    "def main():\n",
    "    #load data\n",
    "    (X_train, y_train), (X_test, y_test) = load_data()\n",
    "    #Sanity checks\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    #Training state initialization\n",
    "    #PRNHKey(0): Seed for JAX's random number generator\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng)\n",
    "    \n",
    "    #Training params\n",
    "    num_epochs = 100\n",
    "    \n",
    "    #Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        state, loss = train_epoch(state, (X_train, y_train))\n",
    "        #Print metrics for every 10 epochs\n",
    "        if epoch%10 == 0:\n",
    "            train_acc = compute_accuracy(\n",
    "                state.params, state.apply_fn, X_train, y_train\n",
    "            )\n",
    "            test_acc = compute_accuracy(\n",
    "                state.params, state.apply_fn, X_test, y_test\n",
    "            )\n",
    "            print(f\"Epoch: {epoch:3d}, loss: {loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_acc:.2f%}, Test Acc: {test_acc:.2f%}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96655d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
